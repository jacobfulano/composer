command: composer composer/examples/run_composer_trainer.py -f /mnt/config/parameters.yaml
gpu_num: 32
gpu_type: a100_40gb
image: mosaicml/pytorch:latest
integrations:
- entity: mosaic-ml
  integration_type: wandb
  project: bert-large-run
- git_branch: dev
  git_repo: mosaicml/composer
  integration_type: git_repo
  pip_install: --user --upgrade -e .[all]
parameters:
  callbacks:
    lr_monitor: null
    speed_monitor: null
  dataloader:
    num_workers: 8
    persistent_workers: true
    pin_memory: true
    prefetch_factor: 2
    timeout: 0
  eval_batch_size: 512
  eval_interval: 1500ba
  evaluators:
  - eval_dataset:
      streaming_c4:
        group_method: truncate
        local: /tmp/mds-cache/mds-c4/
        max_seq_len: 128
        mlm: true
        mlm_probability: 0.15
        remote: s3://allenai-c4/mds/1-gz/
        shuffle: false
        split: val
        tokenizer_name: bert-large-uncased
    label: bert_pre_training
    metric_names:
    - LanguageCrossEntropy
    - MaskedAccuracy
  grad_accum: auto
  grad_clip_norm: -1.0
  loggers:
    object_store:
      object_store_hparams:
        s3:
          bucket: mosaicml-internal-checkpoints-bert
    progress_bar: {}
    wandb:
      log_artifacts: true
      rank_zero_only: true
  max_duration: 275184000sp
  model:
    bert:
      pretrained_model_name: bert-large-uncased
      tokenizer_name: bert-large-uncased
      use_pretrained: false
  optimizers:
    decoupled_adamw:
      betas:
      - 0.9
      - 0.98
      eps: 1.0e-06
      lr: 0.0003
      weight_decay: 1.0e-05
  precision: bf16
  run_name: bert-large2-8192bs-0.0003lr-1e-05wd-0.06durwarmup
  save_artifact_name: jacob-bert-large-sweeps/{run_name}/checkpoints/ep{epoch}-ba{batch}-rank{rank}
  save_folder: bert_checkpoints
  save_interval: 3500ba
  save_num_checkpoints_to_keep: 0
  save_overwrite: true
  schedulers:
    linear_decay_with_warmup:
      alpha_f: 0.02
      t_warmup: 0.06dur
  seed: 2057
  train_batch_size: 8192
  train_dataset:
    streaming_c4:
      group_method: truncate
      local: /tmp/mds-cache/mds-c4/
      max_seq_len: 128
      mlm: true
      mlm_probability: 0.15
      remote: s3://allenai-c4/mds/1-gz/
      shuffle: true
      split: train
      tokenizer_name: bert-large-uncased
platform: r7z2
run_name: bert-large2-8192bs-0.0003lr-1e-05wd-0.06durwarmup
