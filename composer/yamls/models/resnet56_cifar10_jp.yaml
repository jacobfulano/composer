train_dataset:
  cifar10:
    datadir: /datasets/CIFAR10
    is_train: true
    download: false
    shuffle: true
    drop_last: true
val_dataset:
  cifar10:
    datadir: /datasets/CIFAR10
    is_train: false
    download: false
    shuffle: false
    drop_last: false
optimizer:
  decoupled_sgdw:
    lr: 1.2
    momentum: 0.9
    weight_decay: 2.0e-3
schedulers:
  - warmup:
      warmup_iters: "5ep"
      warmup_method: linear
      warmup_factor: 0
      verbose: false
      interval: step
  - multistep:
      milestones:
        - "80ep"
        - "120ep"
      gamma: 0.1
      interval: epoch
model:
  resnet56_cifar10:
    initializers:
      - kaiming_normal
      - bn_uniform
    num_classes: 10
loggers:
  - file:
      log_level: epoch
      filename: stdout
      buffer_size: 1
      flush_every_n_batches: 100
      every_n_epochs: 1
      every_n_batches: 100
max_epochs: 160
total_batch_size: 1024
eval_batch_size: 1000
seed: 17
validate_every_n_epochs: 1
grad_accum: 1
device:
  gpu:
    n_gpus: 1
dataloader:
  pin_memory: true
  timeout: 0
  prefetch_factor: 2
  persistent_workers: true
  num_workers: 8
precision: amp

image: mosaicml/research:latest
parameters:
  instance_types:
  - cota-g8-3080
  seed:
  - 5
  workers_per_gpu:
  - 8

command: >-
  git clone git@github.com:jacobfulano/composer

  cd composer

  git checkout clr_warmup

  pip install -e .
  
  python examples/run_mosaic_trainer.py -f composer/yamls/models/resnet56_cifar10_jp.yaml --loggers.wandb.project jacob_test --loggers.wandb.name test_run
